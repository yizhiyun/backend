version: '3.0'
services:
  spark-master0:
    # build:
    #   context: .
    #   dockerfile: DockerfileWithLivy
    image: hongchhe/sparkwithlivy
    container_name: spark-master0
    hostname: spark-master0
    environment:
      - SPARK_TYPE=master
      - SPARK_MASTER_URL=spark:\/\/spark-master0:7077
      - START_SPARK_HISTORY_SERVER=true
      - SPARK_HISTORYLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/spark-events
      - SPARK_EVENTLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/eventlog
      - WORKER_LIST=spark-master0
      - HDFS_HOST=spark-master0
      - SLAVE_LIST=spark-master0
      - HDFS_WORK_BASE_DIR=\/myvol\/spark-master0
      - HDFS_TYPE=all
      - YARN_TYPE=none
      - HDFS_REPLICA=1
      - START_LIVY_SERVER=true
    volumes:
      - ./myvol:/myvol
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    ports:
      - 8080:8080 # masterWebUI
      - 7077:7077 # master service
      - 6066:6066 # REST server
      - 18080:18080 # history server
      - 4040:4040 # driverProgramWebUI
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 4044:4044
      - 4045:4045
      - 8888:8888 # ipython notebook
      # HDFS ENV
      - 50070:50070 # name node port
      - 9000:9000
      - 8998:8998
      - 50010:50010
      - 50020:50020
      - 50075:50075
      - 8088  # resource manager webapp port
      - 19888 # mapreduce job history webapp server
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 1m30s
      timeout: 10s
      retries: 3
    networks:
      - net_hadoop

networks:
  net_hadoop:
    driver: bridge
